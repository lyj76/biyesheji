# 前馈神经网络均衡器 (FNN) 原理详解

本文档旨在介绍基于前馈神经网络（FNN, Feedforward Neural Network）的非线性均衡技术，并探讨其在高速光通信中的应用。

与传统的 FFE（线性）和 VNLE（固定非线性核）不同，神经网络通过**学习**来寻找最佳的非线性映射关系，具有极强的非线性拟合能力。

---

## 1. 核心理念：从“设计公式”到“学习映射”

### 1.1 传统算法的局限
*   **Linear FFE**: 假设 $y = \mathbf{w}^T \mathbf{x}$。只能处理线性失真（色散、带宽限制）。
*   **Volterra (VNLE)**: 假设 $y = \mathbf{w}_{lin}^T \mathbf{x} + \mathbf{w}_{vol}^T (\mathbf{x} \otimes \mathbf{x})$。虽然引入了非线性，但它是**人为预设**的（比如只包含平方项），且参数量随记忆长度爆炸式增长。

### 1.2 神经网络的突破
根据 **通用近似定理 (Universal Approximation Theorem)**，一个包含足够多神经元的单隐层前馈网络，可以以任意精度逼近任何连续函数。
$$ y = f_{NN}(\mathbf{x}) $$
我们不再假设失真是“平方”还是“立方”，而是把接收到的信号 $\mathbf{x}$ 扔给网络，让它自己去“看”到底发生了什么畸变，并输出干净的信号。

---

## 2. 系统架构

FNN 均衡器本质上是一个**多层感知机 (MLP)**。

### 2.1 输入层 (Input Layer)
*   **输入特征**: 接收端信号的滑动窗口。
*   **窗口大小 (Tap Length)**: 类似于 FFE 的 tap 数。假设输入为 $\mathbf{x}(n) = [x(n-L), ..., x(n), ..., x(n+L)]$。
*   **节点数**: 等于窗口长度（例如 51, 101）。

### 2.2 隐藏层 (Hidden Layers)
这是魔法发生的地方。
*   **线性变换**: $z = \mathbf{W} \mathbf{x} + \mathbf{b}$
*   **激活函数 (Activation Function)**: 引入非线性的关键。
    *   **ReLU (Rectified Linear Unit)**: $f(z) = \max(0, z)$。计算快，梯度好。
    *   **Tanh**: $f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$。输出范围 [-1, 1]，适合信号处理。
    *   **Sigmoid**: 容易梯度消失，现在用得较少。

### 2.3 输出层 (Output Layer)
*   **任务**: 回归 (Regression)。
*   **节点数**: 1 (对应当前时刻的均衡后符号 $y(n)$)。
*   **激活函数**: 通常为 Linear (即 $y = z$)，因为我们要输出连续的幅度值。

---

## 3. 训练过程 (Backpropagation)

与 RLS 逐符号更新不同，神经网络通常采用**批处理 (Batch)** 或 **Mini-batch** 训练。

1.  **前向传播 (Forward Pass)**:
    数据流过网络，计算出预测值 $\hat{y}$。

2.  **计算损失 (Loss Calculation)**:
    通常使用均方误差 (MSE):
    $$ L = \frac{1}{N} \sum (\hat{y} - y_{true})^2 $$

3.  **反向传播 (Backpropagation)**:
    利用链式法则，计算 Loss 对每个权重 $w$ 的梯度 $\frac{\partial L}{\partial w}$。

4.  **参数更新 (Optimizer)**:
    使用梯度下降法更新权重。
    *   **SGD**: $w = w - \eta \cdot \nabla w$
    *   **Adam**: 自适应学习率，收敛更快，是目前的默认首选。

---

## 4. MATLAB 实现细节 (GPU 加速)

为了充分利用计算资源，本项目的实现具有以下特点：

1.  **CUDA 加速**:
    MATLAB 的 Deep Learning Toolbox 底层经过高度优化。如果检测到 GPU，数据会自动转换为 `gpuArray` 或在 `trainNetwork` 中指定 ExecutionEnvironment，从而获得几十倍的加速。

2.  **数据构造**:
    *   输入 $\mathbf{X}$: 这是一个 Matrix，大小为 $[InputSize \times NumSamples]$。每一列代表一个时刻的滑动窗口。
    *   输出 $\mathbf{Y}$: 这是一个 Vector，大小为 $[1 \times NumSamples]$。

3.  **训练/测试切分**:
    *   **Training**: 使用前导码 (Preamble) 进行训练。
    *   **Inference**: 训练完成后，网络权重冻结（或缓慢微调），对后续所有 Payload 数据进行一次性前向推理。

---

## 5. 优缺点总结

| 特性 | 线性 FFE | Volterra | FNN (神经网络) |
| :--- | :--- | :--- | :--- |
| **非线性能力** | 无 | 有限 (预设阶数) | **极强 (自适应)** |
| **计算复杂度** | 低 | 高 (随阶数爆炸) | **中/高 (取决于层数)** |
| **并行度** | 低 (串行依赖) | 低 | **极高 (矩阵乘法)** |
| **收敛速度** | 快 (RLS) | 快 (RLS) | 较慢 (需要 Epochs) |
| **灵活性** | 差 | 一般 | **好** |

---
**Tip:** 在 200Gbps 这样的超高速场景下，FNN 的推理延迟是一个巨大的挑战。通常在离线 DSP 处理中使用 FNN 来探究系统性能极限，或者在硬件上通过剪枝和量化来实现。




你这份笔记**整体方向是对的，而且明显是“懂通信的人写的”**，不是那种套 AI 模板的。但我必须直说一句实话（这是你 BER 上不去的关键）：

> **你这份说明在“原理层面 80–85% 正确”，
> 但在“通信可用性 + BER 能否下限”上，有几处**
> **非常容易把人带沟里的隐性问题。**

下面我会**逐条指出：
✔ 哪些是准确的
⚠ 哪些是“理论对但工程会翻车”
❌ 哪些地方会直接导致 BER 上不去**

并且给你**可操作的修正建议**。

---


# 二、⚠️ “理论没错，但会把 BER 拉垮”的关键问题

下面这几条，**是你 BER 达不到要求的高概率原因**。

---

## ⚠️ 问题 1：你把“回归 MSE ≠ BER”这件事说轻了

你写的是：

```math
L = \frac{1}{N} \sum (\hat{y} - y_{true})^2
```

**问题不在公式本身，而在你默认：**

> MSE 低 → BER 低

### ❌ 在 PAM4 / 非线性信道里，这个推断经常是错的

#### 原因：

* MSE 是 **欧氏距离**
* BER 是 **判决边界错误概率**
* FNN 很容易学成：

  * **整体 MSE 不低**
  * 但 **判决边界歪了**

👉 **BER 会炸**

### 🔧 工程级修正建议（强烈）

你至少要 **做其中一条**：

#### ✔ 方法 A（最常用）

> **输出回归 + 后面加“判决约束”**

* 对输出加 **soft clipping**
* 或训练时：

  * 对靠近判决阈值的样本加权

#### ✔ 方法 B（通信论文里很常见）

> **把问题当成 4-class 分类**

* 输出 4 个节点
* softmax
* cross-entropy
* 再 hard decision

👉 **BER 通常会立刻改善**

---

## ⚠️ 问题 2：你忽略了“标签本身是有噪的”

你默认：

```text
y_true = 发送的 PAM4 符号
```

### ❌ 但在光通信里：

* preamble 本身：

  * 有噪声
  * 有 ISI
  * 有采样偏差

你等于在做：

> **“用噪声标签监督一个强模型”**

FNN 非常容易：

* 学到噪声
* 判决边界抖动
* BER 反而 worse than FFE

### 🔧 修正建议（非常关键）

至少做一条：

* 标签来自：

  * **已知理想符号**
  * 或 FFE / LMS 预均衡输出
* 或：

  * **teacher-student（蒸馏）**

    * FFE → FNN

---

## ⚠️ 问题 3：激活函数选 ReLU 在通信里很危险

你写：

> ReLU：计算快，梯度好

### ❌ 在 PAM4 均衡里，这是**隐形大坑**

#### 原因：

* 信号是：

  * 正负对称
  * 连续幅度
* ReLU：

  * 单边
  * 截断负值
  * 会扭曲幅度分布

👉 很多通信论文 **明确不用 ReLU**

### ✅ 更稳妥的选择

| 激活         | 推荐度    |
| ---------- | ------ |
| tanh       | ⭐⭐⭐⭐⭐  |
| leaky-ReLU | ⭐⭐⭐    |
| ReLU       | ❌（不推荐） |

---

## ⚠️ 问题 4：你低估了“输入归一化”的重要性

你完全没提这一点，但这是 **BER 生死线**。

### 如果你直接喂：

```matlab
x(n) = ADC 输出
```

❌ FNN 会学到：

* 幅度漂移
* ROP 偏移
* DC 偏置

👉 判决门限直接漂

### 🔧 强烈建议

```text
输入 = (x - mean) / std
```

或：

* 固定到 [-1,1]
* per-batch normalization

---

## ⚠️ 问题 5：你没区分“训练收敛”和“通信稳定”

你说：

> Inference: 权重冻结，对 payload 推理

### 但在通信里：

* 光链路是：

  * 漂的
  * 随时间变
  * ROP 在动

FNN：

* 一旦偏离训练点
* BER 会**突然崩**

### 🔧 工程做法

* **少量在线微调**
* 或：

  * 迁移学习
  * 滑窗重训练
  * 混合 FFE + FNN

---

# 三、一个关键判断（你 BER 上不去的最可能原因）

> **不是 GPU、不是算力、不是模型不够大**

而是👇

### ❗你在用：

> **“AI 回归思维”解决“通信判决问题”**

这在 PAM4 非线性均衡里
👉 **是 BER 翻车的第一大原因**

---

# 四、我给你一个“立刻可用的 BER 修正 checklist”

你可以照这个一条条查：

### ✅ 输入是否归一化？

### ✅ 标签是否干净？

### ✅ 激活是否 tanh？

### ✅ 是否考虑分类而非回归？

### ✅ 是否在判决边界附近加权？

### ✅ FNN 前是否有 FFE？

---

# 五、一句真正工程向的建议（很重要）

> **在光通信里：
> FNN ≠ 万能均衡器
> FNN = “非线性补偿模块”**

最稳结构通常是：

```
ADC → FFE → FNN → Hard Decision
```

而不是：

```
ADC → FNN → 判决
```

---

